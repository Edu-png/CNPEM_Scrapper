# -*- coding: utf-8 -*-
"""CNPEM_boot_final

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D9kMCKBb7KJS9sYHG__y9mbYWgdLrmOg
"""

!pip install beautifulsoup4 pandas openpyxl requests

!sudo apt-get install ca-certificates

!pip install --upgrade python

#Agora sim, vamos resgatar as vagas!

import requests
from bs4 import BeautifulSoup
import pandas as pd
import urllib3

# URL da página a ser raspada
url = "https://pages.cnpem.br/trabalheconosco/vagas-abertas/"

# Desativar os avisos de verificação SSL (NÃO recomendado para ambientes de produção)
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Definindo o user agent:
HEADERS = ({
    'User-Agent': 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_1) Gecko/20100101 Firefox/52.2',
    'Accept-Language': 'pt-br, en;q=0.9,*;q=0.8'
})

# Desativar a verificação SSL (NÃO recomendado para ambientes de produção)
response = requests.get(url, verify=False, headers=HEADERS)

# Verificar se a solicitação foi bem-sucedida
if response.status_code == 200:
    soup = BeautifulSoup(response.text, 'html.parser')

    # Encontre todas as entradas de vagas na página
    vagas = soup.find_all('div', class_='post-entry-content')

    # Inicialize listas para armazenar os dados
    titulos = []
    datas = []
    resumos = []
    atividades = []
    requisitos = []
    beneficios = []

    # Loop pelas vagas e extrair informações pela página 1 (única)
    for vaga in vagas:
        titulo = vaga.find('h3').text.strip()
        data = vaga.find('time', class_='entry-date updated').text.strip()
        resumo = vaga.find('div', class_='entry-excerpt').text.strip()

        # Obtenha o link da vaga
        link = vaga.find('a')['href']
        #print(link) #Até aqui ta funcionando tudo bem.

        # Acesse o link da vaga individual
        response_vaga = requests.get(link, verify=False, headers=HEADERS)

        if response_vaga.status_code == 200:
            soup_vaga = BeautifulSoup(response_vaga.text, 'html.parser')

            # Encontre todos os parágrafos na página da vaga
            content = soup_vaga.find("div", {"class": "entry-content"}).text
            paragrafos = soup_vaga.find_all('p')

            # Inicialize variáveis para as informações
            atividade = content.split('Atividades')[1].split('Requisitos')[0]
            requisito = content.split('Requisitos')[1].split('Benefícios')[0]
            beneficio = content.split('Benefícios')[1].split('Envie seu Currículo')[0]

            # Loop pelos parágrafos para encontrar as informações desejadas
            for p in paragrafos:
                if "Atividades:" in p.text:
                    atividade = p.find_next('p').text.strip()
                elif "Requisitos:" in p.text:
                    requisito = p.find_next('p').text.strip()
                elif "Benefícios:" in p.text:
                    beneficio = p.find_next('p').text.strip()

            titulos.append(titulo)
            datas.append(data)
            resumos.append(resumo)
            atividades.append(atividade)
            requisitos.append(requisito)
            beneficios.append(beneficio)

    # Criar um DataFrame do pandas
    data_dict = {
        'Título da Vaga': titulos,
        'Data': datas,
        'Resumo': resumos,
        'Atividades': atividades,
        'Requisitos': requisitos,
        'Beneficios': beneficios
    }

    df = pd.DataFrame(data_dict)
    df = df.apply(lambda x: x.str.replace('\n\n\n\n', ' '))
    df = df.apply(lambda x: x.str.replace('\n\n\n', ' '))
    df = df.apply(lambda x: x.str.replace('\n', ' '))

    print(df)
    # Salvar o DataFrame em um arquivo Excel
    df.to_excel('vagas_cnpem.xlsx', index=False)
else:
    print("Falha ao acessar a página.")